---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

1. Read dataset and remove column
```{r}
# Step 1: Read the dataset
dataset <- read.csv("NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")

# Step 2: Remove the first column (assuming it's unnamed or indexed as V1)
dataset <- dataset[, -1]

# Step 3: Remove the columns related to recidivism arrests in year1, year2, and year3
dataset <- dataset[, !names(dataset) %in% c("Recidivism_Arrest_Year1", "Recidivism_Arrest_Year2", "Recidivism_Arrest_Year3")]

# Display the cleaned dataset
head(dataset)

```

2. Take a summary of the data
```{r}
summary(dataset)


```

3.Identify columns with missing values (including empty strings)
```{r}

# Check for missing values including empty strings
missing_values <- sapply(dataset, function(x) sum(is.na(x) | x == ""))

# Calculate percentages (using nrow(dataset) instead of nrow(df))
missing_percentages <- (missing_values / nrow(dataset)) * 100

# Create a data frame summarizing the results
missing_summary <- data.frame(
  Column = names(dataset),
  MissingCount = missing_values,
  MissingPercentage = missing_percentages
)

# Filter only columns with missing values
missing_summary <- missing_summary[missing_summary$MissingCount > 0, ]

# Display the summary
missing_summary
```

4. Identify categorical variables
```{r}
categorical_vars <- c("Gender", "Race", "Age_at_Release", "Gang_Affiliated", "Supervision_Level_First", "Education_Level", "Prison_Offense", "Prison_Years", "Prior_Arrest_Episodes_DVCharges", "Prior_Arrest_Episodes_GunCharges", "Prior_Conviction_Episodes_Viol", "Prior_Conviction_Episodes_PPViolationCharges", "Prior_Conviction_Episodes_DomesticViolenceCharges", "Prior_Conviction_Episodes_GunCharges", "Prior_Revocations_Parole", "Prior_Revocations_Probation", "Condition_MH_SA", "Condition_Cog_Ed", "Condition_Other", "Violations_ElectronicMonitoring", "Violations_Instruction", "Violations_FailToReport", "Violations_MoveWithoutPermission", "Recidivism_Within_3years")

# Convert categorical variables to factors
dataset[categorical_vars] <- lapply(dataset[categorical_vars], factor)
dataset
```


5.Function to convert text-based numeric representations
```{r}
# Function to convert text-based numeric representations
convert_numeric_vars <- function(data) {
  # Mapping for conversion
  convert_prison_years <- function(x) {
    x <- as.character(x)
    ifelse(grepl("More than 3 years", x), 4,
           ifelse(grepl("Greater than 2 to 3 years", x), 3,
                  ifelse(grepl("1-2 years", x), 2,
                         ifelse(grepl("Less than 1 year", x), 1, NA))))
  }
  
  # Generic function for "X or more" type conversions
  convert_or_more <- function(x) {
    x <- as.character(x)
    # Remove "or more" and convert to numeric
    as.numeric(gsub(" or more", "", x))
  }
  
  # Columns that need conversion
  prison_years_cols <- c("Prison_Years")
  or_more_cols <- c(
    "Prior_Arrest_Episodes_Felony", 
    "Prior_Arrest_Episodes_Misd", 
    "Prior_Arrest_Episodes_Violent", 
    "Prior_Arrest_Episodes_Property", 
    "Prior_Arrest_Episodes_Drug",
    "Delinquency_Reports",
    "Program_Attendances", 
    "Program_UnexcusedAbsences", 
    "Residence_Changes",
    "Avg_Days_per_DrugTest",
    "DrugTests_THC_Positive",
    "DrugTests_Cocaine_Positive", 
    "DrugTests_Meth_Positive", 
    "DrugTests_Other_Positive"
  )
  
  # Apply conversions
  for (col in prison_years_cols) {
    data[[col]] <- convert_prison_years(data[[col]])
  }
  
  for (col in or_more_cols) {
    data[[col]] <- convert_or_more(data[[col]])
  }
  
  return(data)
}

# Number of numeric variables
numeric_vars <- c(
  "Residence_PUMA", "Age_at_Release", "Supervision_Risk_Score_First", 
  "Dependents", "Prison_Years", "Prior_Arrest_Episodes_Felony",
  "Prior_Arrest_Episodes_Misd", "Prior_Arrest_Episodes_Violent", 
  "Prior_Arrest_Episodes_Property", "Prior_Arrest_Episodes_Drug",
  "Delinquency_Reports", "Program_Attendances", 
  "Program_UnexcusedAbsences", "Residence_Changes", 
  "Employment_Exempt", "Avg_Days_per_DrugTest", 
  "DrugTests_THC_Positive", "DrugTests_Cocaine_Positive", 
  "DrugTests_Meth_Positive", "DrugTests_Other_Positive", 
  "Percent_Days_Employed", "Jobs_Per_Year", "Training_Sample"
)

# Print the count of numeric variables
length(numeric_vars)
```
6.Split the data into train and test sets based on "Training_Sample"
```{r}
# Load the dplyr package
library(dplyr)

# Split the data into train and test sets based on "Training_Sample"
train_data <- dataset %>% filter(Training_Sample == 1)
test_data <- dataset %>% filter(Training_Sample == 0)

# Remove the "Training_Sample" variable from both sets
train_data <- train_data %>% select(-Training_Sample)
test_data <- test_data %>% select(-Training_Sample)

# Display the data
test_data
train_data
```

7. missing values handling 
```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# Impute drug-related variables
train_data <- train_data %>%
  mutate_at(vars(starts_with("DrugTests_")), ~replace_na(., 0)) %>%
  mutate(drug_imputed = if_else(all(is.na(.)), TRUE, FALSE))

test_data <- test_data %>%
  mutate_at(vars(starts_with("DrugTests_")), ~replace_na(., 0)) %>%
  mutate(drug_imputed = if_else(all(is.na(.)), TRUE, FALSE))

# Impute other missing values
# Calculate imputation statistics from training data only
mode_imputation <- sapply(train_data, function(x) {
  if (is.factor(x)) {
    names(sort(table(x), decreasing = TRUE))[1]
  } else {
    NA
  }
})

median_imputation <- sapply(train_data, function(x) {
  if (is.numeric(x)) {
    median(x, na.rm = TRUE)
  } else {
    NA
  }
})

# Impute missing values in train and test data
train_data <- train_data %>%
  mutate(across(everything(), 
                ~ifelse(is.na(.), 
                        ifelse(is.factor(.), mode_imputation[cur_column()], median_imputation[cur_column()]), 
                        .)))

test_data <- test_data %>%
  mutate(across(everything(), 
                ~ifelse(is.na(.), 
                        ifelse(is.factor(.), mode_imputation[cur_column()], median_imputation[cur_column()]), 
                        .)))
test_data
train_data

```

8.Creating a Simple Benchmark
```{r}
# Now apply the case_when function
train_data <- train_data %>%
  mutate(risk_group = case_when(
    `Supervision_Risk_Score_First` >= 1 & `Supervision_Risk_Score_First` <= 3 ~ "low",
    `Supervision_Risk_Score_First` >= 4 & `Supervision_Risk_Score_First` <= 6 ~ "medium",
    `Supervision_Risk_Score_First` >= 7 ~ "high",
    TRUE ~ NA_character_
  ))

test_data <- test_data %>%
  mutate(risk_group = case_when(
    `Supervision_Risk_Score_First` >= 1 & `Supervision_Risk_Score_First` <= 3 ~ "low",
    `Supervision_Risk_Score_First` >= 4 & `Supervision_Risk_Score_First` <= 6 ~ "medium",
    `Supervision_Risk_Score_First` >= 7 ~ "high",
    TRUE ~ NA_character_
  ))

# For simplicity, we assume 60% of high-risk people reoffend
benchmark_predictions <- test_data %>%
  mutate(predicted_prob = case_when(
    risk_group == "high" ~ 0.60,  # 60% chance for high risk
    TRUE ~ 0  # 0% for low and medium risk
  )) %>%
  mutate(predicted_class = ifelse(predicted_prob >= 0.5, TRUE, FALSE))

# Create confusion matrix
conf_matrix <- table(Predicted = benchmark_predictions$predicted_class, 
                     Actual = test_data$`Recidivism_Within_3years`)

# Calculate precision, recall, and F1 score for 'Recidivism Within 3years' = TRUE
TP <- conf_matrix["TRUE", "2"]
TN <- conf_matrix["FALSE", "1"]
FP <- conf_matrix["TRUE", "1"]
FN <- conf_matrix["FALSE", "2"]


# Precision (Positive Predictive Value)
precision <- TP / (TP + FP)

# Recall (True Positive Rate)
recall <- TP / (TP + FN)

# F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)

# Output the metrics
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("F1 Score: ", f1_score, "\n")

```
 Training ML Models
 
9. Lasso Logistic Regression model using “glmnet” and
 “caret” as explained in the code demo lectures to predict the “Recidivism Within 3years”. 
```{r}
library(glmnet)
library(caret)

# Set seed for reproducibility
set.seed(2024)

# Define the response variable and predictors
x_train <- model.matrix(`Recidivism_Within_3years` ~ ., data = train_data)[, -1]  # Remove intercept column
y_train <- train_data$`Recidivism_Within_3years`

x_test <- model.matrix(`Recidivism_Within_3years` ~ ., data = test_data)[, -1]  # Predictor matrix
y_test <- test_data$`Recidivism_Within_3years`  # Actual values

# Define the training control for 5-fold cross-validation
train_control <- trainControl(method = "cv", number = 5)

# Train Lasso Logistic Regression (alpha = 1 for Lasso)
lasso_model <- train(
  x = x_train,
  y = as.factor(y_train),
  method = "glmnet",
  trControl = trainControl("cv", number = 5),
  tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(-3, 3, length = 100))
)

# Print the best lambda value
print(lasso_model$bestTune)

# Plot the performance of different lambda values
plot(lasso_model)

```

10.Get the coefficients for the best tuned model in q9. Did Lasso shrink some of
 the coefficients to zero? If so, what does this mean?
```{r}
# Generate predictions
predictions <- predict(lasso_model$finalModel, newx = x_test, s = lasso_model$bestTune$lambda)

# Calculate RMSE
rmse_value <- RMSE(predictions, y_test)

# Output RMSE
cat("RMSE:", rmse_value, "\n")# Extract coefficients for the best tuned model
# Extract the coefficients for the best-tuned lambda
lasso_coefficients <- coef(lasso_model$finalModel, s = lasso_model$bestTune$lambda)

# Print the coefficients
print(lasso_coefficients)

best_lasso_lambda <- lasso_model$bestTune$lambda
lasso_coefficients <- coef(lasso_model$finalModel, s = best_lasso_lambda)

# Print non-zero coefficients
non_zero_coeffs <- lasso_coefficients[lasso_coefficients != 0]
print(non_zero_coeffs)

# Check if Lasso shrank coefficients to zero
if (any(lasso_coefficients == 0)) {
  cat("Lasso shrank some coefficients to zero.\n")
} else {
  cat("Lasso did not shrink any coefficients to zero.\n")
}

```
Lasso shrinking coefficients to zero: This implies that some predictors were deemed unimportant and were effectively removed from the model. Lasso acts as a feature selection method.

11.Again and train a Ridge Logistic Regression model using 5-fold
 cross validation and tune lambda as you did for Lasso in q9.
```{r}
# Set seed for reproducibility
set.seed(2024)

# Train Ridge Logistic Regression (alpha = 0 for Ridge)
ridge_model <- train(
  x = x_train,
  y = as.factor(y_train),
  method = "glmnet",
  trControl = train_control,
  tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))
)

# Print the best lambda value
print(ridge_model$bestTune)

# Plot the performance of different lambda values
plot(ridge_model)

# Generate predictions
predictions <- predict(lasso_model$finalModel, newx = x_test, s = lasso_model$bestTune$lambda)

# Calculate RMSE
rmse_value <- RMSE(predictions, y_test)


```

12. train an Elastic Net Logistic Regression model using
 5-fold cross validation and tune lambda and alpha.
```{r}
# Set seed for reproducibility
set.seed(2024)

# Train Elastic Net Logistic Regression (alpha between 0 and 1)
elastic_net_model <- train(
  x = x_train,
  y = as.factor(y_train),
  method = "glmnet",
  trControl = train_control,
  tuneGrid = expand.grid(alpha =seq(0,1, length=10), lambda = 10^seq(-3, 3, length = 100))
)

# Print the best alpha and lambda values
print(elastic_net_model$bestTune)

# Plot the performance of different alpha and lambda combinations
plot(elastic_net_model)


```

13. use “caret” package with “rf” method to train a random forest
 model (version 2) on the training data to predict “Recidivism Within 3years”. 
```{r}
# Load necessary libraries
library(caret)
library(randomForest)

# Set seed for reproducibility
set.seed(2024)

# Ensure the response variable is a factor with two levels for classification
train_data$Recidivism_Within_3years <- as.factor(train_data$Recidivism_Within_3years)

# Define the train control for 5-fold cross-validation
train_control <- trainControl(
  method = "cv",            # Cross-validation
  number = 5,               # 5 folds
  verboseIter = TRUE,       # Show progress during training
  search = "grid"           # Auto-tune hyperparameters
)

# Train the random forest model with the 'rf' method
rf_model <- train(
  Recidivism_Within_3years ~ .,  # Formula to predict 'Recidivism Within 3years'
  data = train_data,             # Use the training data
  method = "rf",                 # Random forest algorithm
  trControl = train_control,     # Cross-validation setup
  importance = TRUE              # Compute variable importance
)

# Print the trained random forest model
print(rf_model)


```

14.
```{r}
# Extract variable importance
importance_rf <- varImp(rf_model, scale = TRUE)

# Print the variable importance
print(importance_rf)

```

15.
```{r}
# Train the Gradient Boosted Tree (GBM) model
gbm_model <- train(
  Recidivism_Within_3years ~ .,  # Formula to predict 'Recidivism Within 3years'
  data = train_data,             # Use the training data
  method = "gbm",                # Gradient Boosting Model
  trControl = train_control,     # Cross-validation setup
  verbose = FALSE                # Suppress detailed output
)

# Print the trained GBM model
print(gbm_model)


```

16.
```{r}
# Create a list of models
models <- list(Lasso = lasso_model, Ridge = ridge_model, ElasticNet = elastic_net_model, 
               RandomForest = rf_model, GBM = gbm_model)

# Compare the models using resamples
model_comparison <- resamples(models)

# Print the comparison results
summary(model_comparison)

```

# Performance Metrics Summary

# Accuracy Comparison:
# - Gradient Boosted Machine (GBM):
#   - Achieves the highest mean accuracy of 0.7355.
#   - Has a narrow range, indicating strong stability across resamples.
# - Random Forest:
#   - Mean accuracy of 0.7283, slightly lower than GBM.
#   - Shows slightly wider variability compared to GBM.
# - Linear Models (LASSO, Ridge, Elastic Net):
#   - Mean accuracies hover around 0.716.
#   - Struggle with capturing complex patterns in the dataset due to their linear nature.

# Kappa Comparison:
# - GBM:
#   - Highest mean Kappa of 0.4445, showcasing superior handling of imbalanced data.
# - Random Forest:
#   - Kappa of 0.4286, indicating strong performance but slightly less effective than GBM.
# - Linear Models (LASSO, Ridge, Elastic Net):
#   - Mean Kappa values around 0.406.
#   - Weaker at predicting the minority class compared to tree-based methods.

# Conclusion:
# - GBM and Random Forest outperform linear models in both accuracy and Kappa.
# - GBM edges out Random Forest with slightly better performance metrics and stability.
# - Tree-based methods (GBM, Random Forest) are better suited for capturing non-linear relationships and interactions within the dataset.

17.
```{r}
set.seed(2025)

evaluate_model <- function(preds, actuals) {
  preds <- factor(preds, levels = c("false", "true"))
  actuals <- factor(actuals, levels = c("false", "true"))
  conf <- table(Predicted = preds, Actual = actuals)
  print(conf)
  
  TP <- conf["true", "true"]
  FP <- conf["true", "false"]
  FN <- conf["false", "true"]
  
  precision <- TP / (TP + FP) 
  recall <- TP / (TP + FN)
  f1 <- 2 * precision * recall / (precision + recall)
  
  return(list(
    Confusion_Matrix = conf,
    Precision = precision,
    Recall = recall,
    F1_Score = f1
  ))
}

model_results <- list()

cat("\n--- LASSO Model ---\n")

lasso_preds_prob <- as.vector(predict(lasso_model$finalModel, newx = x_test, s = lasso_model$bestTune$lambda, type = "response"))
lasso_preds <- ifelse(lasso_preds_prob > 0.5, "true", "false")
lasso_metrics <- evaluate_model(lasso_preds, test_data$Recidivism_Within_3years)
cat("Precision:", round(lasso_metrics$Precision, 4), 
    "Recall:", round(lasso_metrics$Recall, 4), 
    "F1:", round(lasso_metrics$F1_Score, 4), "\n")
model_results[["LASSO"]] <- lasso_metrics

cat("\n--- Ridge Model ---\n")

ridge_preds_prob <- as.vector(predict(ridge_model$finalModel, newx = x_test, s = ridge_model$bestTune$lambda, type = "response"))
ridge_preds <- ifelse(ridge_preds_prob > 0.5, "true", "false")
ridge_metrics <- evaluate_model(ridge_preds, test_data$Recidivism_Within_3years)
cat("Precision:", round(ridge_metrics$Precision, 4), 
    "Recall:", round(ridge_metrics$Recall, 4), 
    "F1:", round(ridge_metrics$F1_Score, 4), "\n")
model_results[["Ridge"]] <- ridge_metrics

cat("\n--- Elastic Net Model ---\n")

en_preds_prob <- as.vector(predict(elastic_net_model$finalModel, newx = x_test, s = elastic_net_model$bestTune$lambda, type = "response"))
en_preds <- ifelse(en_preds_prob > 0.5, "true", "false")
en_metrics <- evaluate_model(en_preds, test_data$Recidivism_Within_3years)
cat("Precision:", round(en_metrics$Precision, 4), 
    "Recall:", round(en_metrics$Recall, 4), 
    "F1:", round(en_metrics$F1_Score, 4), "\n")
model_results[["ElasticNet"]] <- en_metrics


cat("\n--- Random Forest Model ---\n")
rf_preds <- predict(rf_model, newdata = test_data)
rf_metrics <- evaluate_model(rf_preds, test_data$Recidivism_Within_3years)
cat("Precision:", round(rf_metrics$Precision, 4), 
    "Recall:", round(rf_metrics$Recall, 4), 
    "F1:", round(rf_metrics$F1_Score, 4), "\n")
model_results[["RandomForest"]] <- rf_metrics
  
cat("\n--- GBM Model ---\n")
gbm_preds <- predict(gbm_model, newdata = test_data)
gbm_metrics <- evaluate_model(gbm_preds, test_data$Recidivism_Within_3years)
cat("Precision:", round(gbm_metrics$Precision, 4), 
    "Recall:", round(gbm_metrics$Recall, 4), 
    "F1:", round(gbm_metrics$F1_Score, 4), "\n")
model_results[["GBM"]] <- gbm_metrics

```
Comparison to Heuristic Benchmark:

a. Precision: All five machine learning models (especially GBM with a precision of 0.743) outperform the heuristic benchmark's precision of 0.5780. The ML models correctly predict a much higher proportion of true positives (individuals who reoffend) compared to the heuristic model, which has a relatively lower precision.

b. Recall: The GBM model leads with the highest recall of 0.845, while the heuristic benchmark has a recall of 0.5883. This suggests that the ML models are significantly better at identifying individuals who will recidivate (true positives). The heuristic model misses a substantial number of these individuals because it is based solely on the "risk score" without considering other predictive features.

c. F1 Score: The GBM model has the highest F1 score of 0.7907, which is a clear improvement over the heuristic benchmark's F1 score of 0.5831. The F1 score balances both precision and recall, and the ML models (such as Random Forest, ElasticNet, and GBM) provide a much better balance than the heuristic model.

The machine learning models are trained on historical data, which helps them recognize patterns in recidivism that a simple heuristic model could miss. The heuristic model, while simple, is far less accurate because it relies on a basic rule without accounting for the complexities captured by the machine learning models. The machine learning models make use of multiple features beyond just the risk score.

---

## 18. Bias in Historical Data and KNN Classifiers

### Issues with Bias:
Building a K Nearest Neighbor (KNN) classifier using historical recidivism data risks perpetuating systemic biases embedded in the criminal justice system. These biases may originate from:
- **Disproportionate Policing and Arrests:** Historical data may disproportionately represent marginalized groups due to systemic bias in law enforcement, even if these groups are not inherently more likely to reoffend.
- **Socioeconomic Disparities:** Features like employment status and socioeconomic background might correlate with higher recidivism rates due to structural inequalities, not individual behavior.
- **Feedback Loops:** Predictions based on biased data could create self-reinforcing cycles, where affected groups are denied parole or services, further validating the model's biased assumptions.

### Long-Term Societal Impacts:
- **Institutionalized Discrimination:** A biased model could amplify systemic discrimination, disproportionately affecting marginalized groups.
- **Reduced Trust in Justice:** Perceptions of unfairness could erode public trust in the justice system, especially among communities that experience the most harm.
- **Barriers to Reintegration:** Individuals flagged as high risk may face obstacles in accessing rehabilitation programs, employment, and community support, perpetuating cycles of poverty and incarceration.
- **Exacerbation of Inequality:** Biased decisions could worsen socioeconomic and racial disparities, reinforcing the conditions that contribute to recidivism.

### Mitigation Strategies:
- Conduct bias audits to assess disparities across demographic groups.
- Implement fairness-aware algorithms to adjust predictions and counteract biases.
- Ensure transparency in the model's design and decision-making processes.
- Incorporate diverse perspectives, including input from marginalized communities, to ensure ethical AI use in sensitive domains like criminal justice.

---

## 19. Balancing Recidivism Reduction and Fairness

### Key Trade-Offs:
1. **Reducing Recidivism Rates:**
   - An efficient model can lower recidivism rates, reducing crime and enhancing community safety.
   - Cost savings from fewer incarcerations and improved public safety could benefit society broadly.

2. **Ensuring Fairness and Equity:**
   - If the model unfairly penalizes marginalized groups, it risks perpetuating systemic inequalities and harming already disadvantaged populations.
   - Unfairness could undermine public trust in the justice system and exacerbate societal harms.

### Steps for Impact Analysis:
1. **Evaluate Predictive Performance:** Analyze accuracy, false positives, and false negatives across demographic groups to identify disparities.
2. **Assess Social Impact:** Measure societal benefits (e.g., reduced recidivism, cost savings) against harms caused by potential unfair treatment.
3. **Fairness Testing:** Use fairness metrics (e.g., demographic parity, equalized odds) to quantify and address biases.
4. **Engage Stakeholders:** Incorporate feedback from affected communities, civil rights groups, and policymakers to understand broader societal values.
5. **Run Counterfactual Simulations:** Test fairness-aware algorithms or adjust feature weights to assess their impact on metrics like recidivism rates.
6. **Longitudinal Impact Study:** Monitor the model's long-term effects on recidivism rates, parole decisions, and social equity, and recalibrate as needed.

### Decision Framework:
The decision to deploy or adjust the model should consider:
- **Effectiveness:** How well the model reduces recidivism.
- **Fairness:** Whether disparities can be minimized without significantly compromising effectiveness.
- **Ethics:** The societal implications of prioritizing efficiency over equity.

### Mitigation Strategies:
- Adjust algorithms to include fairness constraints or reweight sensitive features.
- Introduce human oversight to correct biased predictions.
- Address systemic inequities in the training data through reforms.

Balancing these trade-offs ensures responsible deployment of AI in high-stakes societal applications, aligning efficiency with ethical considerations.