---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

1.Read dataset and remove column
```{r}
# Step 1: Read the dataset
dataset <- read.csv("NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")

# Step 2: Remove the first column (assuming it's unnamed or indexed as V1)
dataset <- dataset[, -1]

# Step 3: Remove the columns related to recidivism arrests in year1, year2, and year3
dataset <- dataset[, !names(dataset) %in% c("Recidivism_Arrest_Year1", "Recidivism_Arrest_Year2", "Recidivism_Arrest_Year3")]

# Display the cleaned dataset
head(dataset)

```
2: Take a summary of the data
```{r}
summary(dataset)


```
3.Identify columns with missing values (including empty strings)
```{r}

# Check for missing values including empty strings
missing_values <- sapply(dataset, function(x) sum(is.na(x) | x == ""))
missing_percentages <- (missing_values / nrow(df)) * 100

# Create a data frame summarizing the results
missing_summary <- data.frame(
  Column = names(dataset),
  MissingCount = missing_values,
  MissingPercentage = missing_percentages
)

# Filter only columns with missing values
missing_summary <- missing_summary[missing_summary$MissingCount > 0, ]

# Display the summary
missing_summary
```
4. Identify categorical variables
```{r}
categorical_vars <- c("Gender", "Race", "Age_at_Release", "Gang_Affiliated", "Supervision_Level_First", "Education_Level", "Prison_Offense", "Prison_Years", "Prior_Arrest_Episodes_DVCharges", "Prior_Arrest_Episodes_GunCharges", "Prior_Conviction_Episodes_Viol", "Prior_Conviction_Episodes_PPViolationCharges", "Prior_Conviction_Episodes_DomesticViolenceCharges", "Prior_Conviction_Episodes_GunCharges", "Prior_Revocations_Parole", "Prior_Revocations_Probation", "Condition_MH_SA", "Condition_Cog_Ed", "Condition_Other", "Violations_ElectronicMonitoring", "Violations_Instruction", "Violations_FailToReport", "Violations_MoveWithoutPermission", "Recidivism_Within_3years")

# Convert categorical variables to factors
dataset[categorical_vars] <- lapply(dataset[categorical_vars], factor)
dataset
```


5.Function to convert text-based numeric representations
```{r}
# Function to convert text-based numeric representations
convert_numeric_vars <- function(data) {
  # Mapping for conversion
  convert_prison_years <- function(x) {
    x <- as.character(x)
    ifelse(grepl("More than 3 years", x), 4,
           ifelse(grepl("Greater than 2 to 3 years", x), 3,
                  ifelse(grepl("1-2 years", x), 2,
                         ifelse(grepl("Less than 1 year", x), 1, NA))))
  }
  
  # Generic function for "X or more" type conversions
  convert_or_more <- function(x) {
    x <- as.character(x)
    # Remove "or more" and convert to numeric
    as.numeric(gsub(" or more", "", x))
  }
  
  # Columns that need conversion
  prison_years_cols <- c("Prison_Years")
  or_more_cols <- c(
    "Prior_Arrest_Episodes_Felony", 
    "Prior_Arrest_Episodes_Misd", 
    "Prior_Arrest_Episodes_Violent", 
    "Prior_Arrest_Episodes_Property", 
    "Prior_Arrest_Episodes_Drug",
    "Delinquency_Reports",
    "Program_Attendances", 
    "Program_UnexcusedAbsences", 
    "Residence_Changes",
    "Avg_Days_per_DrugTest",
    "DrugTests_THC_Positive",
    "DrugTests_Cocaine_Positive", 
    "DrugTests_Meth_Positive", 
    "DrugTests_Other_Positive"
  )
  
  # Apply conversions
  for (col in prison_years_cols) {
    data[[col]] <- convert_prison_years(data[[col]])
  }
  
  for (col in or_more_cols) {
    data[[col]] <- convert_or_more(data[[col]])
  }
  
  return(data)
}

# Number of numeric variables
numeric_vars <- c(
  "Residence_PUMA", "Age_at_Release", "Supervision_Risk_Score_First", 
  "Dependents", "Prison_Years", "Prior_Arrest_Episodes_Felony",
  "Prior_Arrest_Episodes_Misd", "Prior_Arrest_Episodes_Violent", 
  "Prior_Arrest_Episodes_Property", "Prior_Arrest_Episodes_Drug",
  "Delinquency_Reports", "Program_Attendances", 
  "Program_UnexcusedAbsences", "Residence_Changes", 
  "Employment_Exempt", "Avg_Days_per_DrugTest", 
  "DrugTests_THC_Positive", "DrugTests_Cocaine_Positive", 
  "DrugTests_Meth_Positive", "DrugTests_Other_Positive", 
  "Percent_Days_Employed", "Jobs_Per_Year", "Training_Sample"
)

# Print the count of numeric variables
length(numeric_vars)
```
6.Split the data into train and test sets based on "Training_Sample"
```{r}
train_data <- dataset %>% filter(Training_Sample == 1)
test_data <- dataset %>% filter(Training_Sample == 0)

# Remove the "Training_Sample" variable from both sets
train_data <- train_data %>% select(-Training_Sample)
test_data <- test_data %>% select(-Training_Sample)
test_data
train_data
```
7. missing values handling 
```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# Impute drug-related variables
train_data <- train_data %>%
  mutate_at(vars(starts_with("DrugTests_")), ~replace_na(., 0)) %>%
  mutate(drug_imputed = if_else(all(is.na(.)), TRUE, FALSE))

test_data <- test_data %>%
  mutate_at(vars(starts_with("DrugTests_")), ~replace_na(., 0)) %>%
  mutate(drug_imputed = if_else(all(is.na(.)), TRUE, FALSE))

# Impute other missing values
# Calculate imputation statistics from training data only
mode_imputation <- sapply(train_data, function(x) {
  if (is.factor(x)) {
    names(sort(table(x), decreasing = TRUE))[1]
  } else {
    NA
  }
})

median_imputation <- sapply(train_data, function(x) {
  if (is.numeric(x)) {
    median(x, na.rm = TRUE)
  } else {
    NA
  }
})

# Impute missing values in train and test data
train_data <- train_data %>%
  mutate(across(everything(), 
                ~ifelse(is.na(.), 
                        ifelse(is.factor(.), mode_imputation[cur_column()], median_imputation[cur_column()]), 
                        .)))

test_data <- test_data %>%
  mutate(across(everything(), 
                ~ifelse(is.na(.), 
                        ifelse(is.factor(.), mode_imputation[cur_column()], median_imputation[cur_column()]), 
                        .)))
test_data
train_data

```
8.Creating a Simple Benchmark
```{r}
# Now apply the case_when function
train_data <- train_data %>%
  mutate(risk_group = case_when(
    `Supervision_Risk_Score_First` >= 1 & `Supervision_Risk_Score_First` <= 3 ~ "low",
    `Supervision_Risk_Score_First` >= 4 & `Supervision_Risk_Score_First` <= 6 ~ "medium",
    `Supervision_Risk_Score_First` >= 7 ~ "high",
    TRUE ~ NA_character_
  ))

test_data <- test_data %>%
  mutate(risk_group = case_when(
    `Supervision_Risk_Score_First` >= 1 & `Supervision_Risk_Score_First` <= 3 ~ "low",
    `Supervision_Risk_Score_First` >= 4 & `Supervision_Risk_Score_First` <= 6 ~ "medium",
    `Supervision_Risk_Score_First` >= 7 ~ "high",
    TRUE ~ NA_character_
  ))

# For simplicity, we assume 60% of high-risk people reoffend
benchmark_predictions <- test_data %>%
  mutate(predicted_prob = case_when(
    risk_group == "high" ~ 0.60,  # 60% chance for high risk
    TRUE ~ 0  # 0% for low and medium risk
  )) %>%
  mutate(predicted_class = ifelse(predicted_prob >= 0.5, TRUE, FALSE))

# Create confusion matrix
conf_matrix <- table(Predicted = benchmark_predictions$predicted_class, 
                     Actual = test_data$`Recidivism_Within_3years`)

# Calculate precision, recall, and F1 score for 'Recidivism Within 3years' = TRUE
TP <- conf_matrix["TRUE", "2"]
TN <- conf_matrix["FALSE", "1"]
FP <- conf_matrix["TRUE", "1"]
FN <- conf_matrix["FALSE", "2"]


# Precision (Positive Predictive Value)
precision <- TP / (TP + FP)

# Recall (True Positive Rate)
recall <- TP / (TP + FN)

# F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)

# Output the metrics
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("F1 Score: ", f1_score, "\n")

```
 Training ML Models
9. Lasso Logistic Regression model using “glmnet” and
 “caret” as explained in the code demo lectures to predict the “Recidivism Within 3years”. 
```{r}
library(glmnet)
library(caret)

# Set seed for reproducibility
set.seed(2024)

# Define the response variable and predictors
x_train <- model.matrix(`Recidivism_Within_3years` ~ ., data = train_data)[, -1]  # Remove intercept column
y_train <- train_data$`Recidivism_Within_3years`

x_test <- model.matrix(`Recidivism_Within_3years` ~ ., data = test_data)[, -1]  # Predictor matrix
y_test <- test_data$`Recidivism_Within_3years`  # Actual values

# Define the training control for 5-fold cross-validation
train_control <- trainControl(method = "cv", number = 5)

# Train Lasso Logistic Regression (alpha = 1 for Lasso)
lasso_model <- train(
  x = x_train,
  y = as.factor(y_train),
  method = "glmnet",
  trControl = trainControl("cv", number = 5),
  tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(-3, 3, length = 100))
)

# Print the best lambda value
print(lasso_model$bestTune)

# Plot the performance of different lambda values
plot(lasso_model)

```
10.Get the coefficients for the best tuned model in q9. Did Lasso shrink some of
 the coefficients to zero? If so, what does this mean?
```{r}
# Generate predictions
predictions <- predict(lasso_model$finalModel, newx = x_test, s = lasso_model$bestTune$lambda)

# Calculate RMSE
rmse_value <- RMSE(predictions, y_test)

# Output RMSE
cat("RMSE:", rmse_value, "\n")# Extract coefficients for the best tuned model
# Extract the coefficients for the best-tuned lambda
lasso_coefficients <- coef(lasso_model$finalModel, s = lasso_model$bestTune$lambda)

# Print the coefficients
print(lasso_coefficients)

best_lasso_lambda <- lasso_model$bestTune$lambda
lasso_coefficients <- coef(lasso_model$finalModel, s = best_lasso_lambda)

# Print non-zero coefficients
non_zero_coeffs <- lasso_coefficients[lasso_coefficients != 0]
print(non_zero_coeffs)

# Check if Lasso shrank coefficients to zero
if (any(lasso_coefficients == 0)) {
  cat("Lasso shrank some coefficients to zero.\n")
} else {
  cat("Lasso did not shrink any coefficients to zero.\n")
}

```
Lasso shrinking coefficients to zero: This implies that some predictors were deemed unimportant and were effectively removed from the model. Lasso acts as a feature selection method.

11.Again and train a Ridge Logistic Regression model using 5-fold
 cross validation and tune lambda as you did for Lasso in q9.
```{r}
# Set seed for reproducibility
set.seed(2024)

# Train Ridge Logistic Regression (alpha = 0 for Ridge)
ridge_model <- train(
  x = x_train,
  y = as.factor(y_train),
  method = "glmnet",
  trControl = train_control,
  tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))
)

# Print the best lambda value
print(ridge_model$bestTune)

# Plot the performance of different lambda values
plot(ridge_model)

# Generate predictions
predictions <- predict(lasso_model$finalModel, newx = x_test, s = lasso_model$bestTune$lambda)

# Calculate RMSE
rmse_value <- RMSE(predictions, y_test)


```
12. train an Elastic Net Logistic Regression model using
 5-fold cross validation and tune lambda and alpha.
```{r}
# Set seed for reproducibility
set.seed(2024)

# Train Elastic Net Logistic Regression (alpha between 0 and 1)
elastic_net_model <- train(
  x = x_train,
  y = as.factor(y_train),
  method = "glmnet",
  trControl = train_control,
  tuneGrid = expand.grid(alpha =seq(0,1, length=10), lambda = 10^seq(-3, 3, length = 100))
)

# Print the best alpha and lambda values
print(elastic_net_model$bestTune)

# Plot the performance of different alpha and lambda combinations
plot(elastic_net_model)


```
13. use “caret” package with “rf” method to train a random forest
 model (version 2) on the training data to predict “Recidivism Within 3years”. 
```{r}
# Load necessary libraries
library(caret)
library(randomForest)

# Set seed for reproducibility
set.seed(2024)

# Ensure the response variable is a factor with two levels for classification
train_data$Recidivism_Within_3years <- as.factor(train_data$Recidivism_Within_3years)

# Define the train control for 5-fold cross-validation
train_control <- trainControl(
  method = "cv",            # Cross-validation
  number = 5,               # 5 folds
  verboseIter = TRUE,       # Show progress during training
  search = "grid"           # Auto-tune hyperparameters
)

# Train the random forest model with the 'rf' method
rf_model <- train(
  Recidivism_Within_3years ~ .,  # Formula to predict 'Recidivism Within 3years'
  data = train_data,             # Use the training data
  method = "rf",                 # Random forest algorithm
  trControl = train_control,     # Cross-validation setup
  importance = TRUE              # Compute variable importance
)

# Print the trained random forest model
print(rf_model)


```
```{r}
# Extract variable importance
importance_rf <- varImp(rf_model, scale = TRUE)

# Print the variable importance
print(importance_rf)

```
```{r}
# Train the Gradient Boosted Tree (GBM) model
gbm_model <- train(
  Recidivism_Within_3years ~ .,  # Formula to predict 'Recidivism Within 3years'
  data = train_data,             # Use the training data
  method = "gbm",                # Gradient Boosting Model
  trControl = train_control,     # Cross-validation setup
  verbose = FALSE                # Suppress detailed output
)

# Print the trained GBM model
print(gbm_model)


```
```{r}
# Create a list of models
models <- list(Lasso = lasso_model, Ridge = ridge_model, ElasticNet = elastic_net_model, 
               RandomForest = rf_model, GBM = gbm_model)

# Compare the models using resamples
model_comparison <- resamples(models)

# Print the comparison results
summary(model_comparison)

```
Based on the summary of the cross-validation metrics for the five models:

Accuracy:
Best Performing Models:
GBM has the highest mean accuracy (0.7355) and a narrow range, indicating it is both accurate and stable across resamples.
Random Forest is close behind with a mean accuracy of 0.7283, slightly wider variability compared to GBM.
Lower Performing Models:
LASSO, Ridge, and Elastic Net have similar mean accuracies (~0.716), indicating their linear nature struggles to capture complex patterns in the data.
Kappa:
Best Performing Models:
GBM again leads with the highest mean Kappa (0.4445), indicating its superior ability to handle imbalanced data.
Random Forest follows with a mean Kappa of 0.4286, showing it performs well but is slightly less effective than GBM.
Lower Performing Models:
LASSO, Ridge, and Elastic Net have lower mean Kappa values (~0.406), reflecting their weaker performance in predicting the minority class.
Conclusion:
GBM and Random Forest outperform linear models (LASSO, Ridge, Elastic Net) in both accuracy and Kappa, with GBM having the edge.
The results suggest that tree-based methods better capture the non-linear relationships and interactions in the dataset.
```{r}
# Function to calculate Precision, Recall, and F1 Score
calculate_metrics <- function(model, test_data, threshold = 0.5) {
  predictions <- predict(model, newdata = test_data)  # Get model predictions
  confusion_matrix <- table(Predicted = predictions, Actual = test_data$`Recidivism_Within_3years`)
  
  # Calculate Precision, Recall, and F1 Score for 'Recidivism Within 3years = TRUE'
  # Calculate precision, recall, and F1 score for 'Recidivism Within 3years' = TRUE
  TP <- conf_matrix["TRUE", "2"]
  TN <- conf_matrix["FALSE", "1"]
  FP <- conf_matrix["TRUE", "1"]
  FN <- conf_matrix["FALSE", "2"]
  
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  return(list(Precision = precision, Recall = recall, F1_Score = f1_score))
}

# Calculate metrics for each model on the test data
rf_metrics <- calculate_metrics(rf_model, test_data)
gbm_metrics <- calculate_metrics(gbm_model, test_data)

# Print the metrics
cat("Random Forest Metrics:\n")
print(rf_metrics)

cat("\nGradient Boosted Tree Metrics:\n")
print(gbm_metrics)


```
Based on the metrics for both Random Forest and Gradient Boosted Tree:

Precision: Both models have a precision of 0.6558, meaning they are fairly accurate when predicting the positive class (Recidivism Within 3 years = true), with about 66% of their positive predictions being correct.

Recall: Both models have a recall of 0.4977, indicating that they identify about 50% of the actual positive cases. This is relatively low, meaning they miss half of the true positive cases.

F1 Score: Both models show an F1 score of 0.5659, which balances precision and recall. This score suggests a moderate ability to correctly classify the positive class, but there's room for improvement.

Comparison with Heuristic Benchmark:
If your heuristic benchmark (from Q8) was based on a simple model (e.g., predicting the majority class or random prediction), both the Random Forest and Gradient Boosted Tree models significantly outperform the benchmark, especially in terms of precision and recall, as they provide more meaningful predictions rather than defaulting to a single class.
If the benchmark was a basic model with high precision (but low recall), both models may still offer better recall and F1, suggesting that while they miss some positives (as indicated by recall), they do much better than the benchmark in capturing more true positives and balancing precision.
Conclusion:
Random Forest and Gradient Boosted Tree perform similarly in terms of precision, recall, and F1 score.
Both models outperform the heuristic benchmark in most aspects, but they could still benefit from further tuning, such as hyperparameter optimization or feature engineering, to boost recall and reduce false negatives.




18.


historical recidivism data in building a K Nearest Neighbor (KNN) classifier can perpetuate existing biases against marginalized groups because historical data often reflects systemic inequities in the criminal justice system. These inequities can arise from factors such as discriminatory policing practices, socioeconomic disadvantages, and unequal access to legal representation. If these biases are embedded in the historical data, the KNN model may learn and replicate them, unfairly associating certain demographic features (like socioeconomic background or age) with higher recidivism risk.

Potential Issues with Bias
Disproportionate Policing and Arrests:

Historical data might overrepresent marginalized groups due to systemic bias in law enforcement, even if individuals from these groups are not inherently more likely to reoffend.
Socioeconomic Disparities:

Features like employment status and socioeconomic background might correlate with higher recidivism rates due to structural inequalities rather than individual tendencies.
Feedback Loops:

Using biased predictions to influence parole decisions could create a self-reinforcing cycle, where marginalized individuals are denied parole or services based on predictions, leading to outcomes that further validate the biased model.
Long-Term Societal Impacts
Institutionalized Discrimination:

Widespread use of such a model could institutionalize and amplify existing biases, leading to systematic discrimination against marginalized groups.
Reduced Trust in the Justice System:

A predictive system perceived as unfair could erode public trust in the fairness and integrity of the justice system, particularly among affected communities.
Barrier to Reintegration:

Individuals flagged as high risk by the model might face additional hurdles in accessing rehabilitation programs, employment opportunities, and community support, perpetuating cycles of incarceration and poverty.
Exacerbation of Inequality:

Decisions driven by biased predictions could worsen socioeconomic and racial disparities, reinforcing the very conditions that contribute to higher recidivism.
Mitigation Strategies
To mitigate these issues, steps should include:

Bias Audits: Regularly assess the data and model outputs for disparities across demographic groups.
Fairness Constraints: Introduce fairness-aware algorithms that adjust predictions to counteract identified biases.
Transparent Development: Ensure the model’s design and decision-making processes are transparent and open to scrutiny.
Inclusive Policies: Incorporate input from diverse stakeholders, including representatives from marginalized groups.
Addressing these challenges is crucial to ensure the ethical use of AI systems in sensitive domains like criminal justice.


19.

The trade-off between reducing recidivism rates and ensuring fairness or equity reflects a classic ethical dilemma in deploying AI systems in societal decision-making. While the model’s efficiency in lowering recidivism rates is a positive outcome, the potential reinforcement of systemic biases against certain demographics raises concerns about equity and social justice. Balancing these priorities requires a nuanced analysis of both the benefits and the costs of the model’s implementation.

Key Aspects of the Trade-off

Reducing Recidivism Rates:
A more efficient parole decision process may lead to fewer repeat offenses, reducing crime and enhancing community safety.
The cost savings from fewer incarcerations and improved public safety could benefit society as a whole.

Ensuring Fairness and Equity:
If the model unfairly penalizes marginalized groups, it could perpetuate structural inequalities, further disadvantaging those already disproportionately impacted by the criminal justice system.
Such unfairness could undermine public trust in the justice system and create long-term societal harms.
Analyzing Data and Conducting an Impact Assessment
To assess whether the model’s benefits outweigh its costs, the following steps can be taken:

1. Evaluate Predictive Performance by Demographic Groups
Analyze model accuracy, false positives, and false negatives across different demographic groups (e.g., race, socioeconomic status, age).
Check for significant disparities, such as higher false positive rates for certain groups.
2. Assess Overall Social Impact
Measure the societal benefits of reduced recidivism, including cost savings, community safety improvements, and decreased incarceration rates.
Compare these benefits with the societal harms caused by potential unfair treatment of specific groups.
3. Conduct Fairness Testing
Use fairness metrics like demographic parity, equalized odds, or disparate impact to identify and quantify biases in the model’s predictions.
Assess whether the model meets acceptable thresholds for fairness.
4. Engage Stakeholders
Include input from affected communities, civil rights groups, policymakers, and legal experts to understand broader societal concerns and values.
Consider qualitative feedback alongside quantitative metrics to assess the system’s real-world impact.
5. Run Counterfactual Simulations
Simulate scenarios with fairness-aware algorithms or adjusted weights for sensitive features.
Compare outcomes to determine whether fairness adjustments significantly affect recidivism rates or other key metrics.
6. Longitudinal Impact Study
Monitor the long-term effects of the model on recidivism rates, parole decisions, and social equity.
Regularly update and recalibrate the model as new data and insights become available.
Decision Framework
The final decision on whether to use or adjust the model should weigh:

Effectiveness: How much the model reduces recidivism.
Fairness: Whether disparities in predictions can be minimized without significantly compromising effectiveness.
Ethics: The broader societal implications of prioritizing efficiency over equity, or vice versa.
Mitigation Strategies

If the model is found to reinforce biases:
Adjust the algorithm to include fairness constraints or weights.
Provide supplementary tools or human oversight to correct biased decisions.
Invest in systemic reforms, such as addressing inequities in the training data.
Balancing these trade-offs ensures a responsible approach to deploying AI systems in high-stakes societal applications.

